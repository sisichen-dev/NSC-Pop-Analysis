sampleorder = [1,4,7,2,5,8,3,6,9,38,34,40,39,28,22,...
  1,13,10,2,14,11,3,15,12,33,...
  16,17,18,32,...
  3,...
  25,35,31,29,23,...
  26,27,30,29,20,...
  19,21,24,23,20];

load('neural_data.mat')

[num_type, num_round] = size(countsn'); 
num_input = size(signalsx, 2) + 1; % the extra dimension is the constant term in the model 

rng(0)
data_input = cat(1, signalsx', ones(1, num_round)); % all ones vector for the constant term 
sample_dist = countsn' ./ sum(countsn, 2)'; % distribution of samples 

cur_beta = zeros(num_type, num_input); % initial value of beta 
step_sz = 1;
num_step = 200;
rec_grad = zeros(num_step, 1); % record gradient for record

for ii = 1: 200
    cstep_sz = step_sz / sqrt(ii); % stepsize decay as the square root of steps
    cur_grad = grad_beta(sample_dist, data_input, cur_beta); % compute gradient of log likelihood
    cur_beta = cur_beta + cstep_sz * cur_grad; % gradient ascent update 
    rec_grad(ii) = norm(cur_grad);
end

para_dist = model_dist(cur_beta, data_input); % distribution generated by learned parameters

figure
plot(rec_grad)
xlabel('step')
ylabel('gradient')

figure
for ii = 1: 3
    subplot(3, 1, ii)
    plot(para_dist(ii, sampleorder), 'DisplayName', 'Model')
    hold on    
    plot(sample_dist(ii, sampleorder), 'DisplayName', 'Data')
    hold off
    ylim([0, 1])
    title(['cell type', num2str(ii)])
    xlabel('condition index')
    ylabel('proportions')
    legend()
end

% mean absolute error 
mean(mean((abs(para_dist - sample_dist))))



grow_vecn =  data_input' \ log(countsn);
data_pred = (data_input' * grow_vecn)';
figure
for ii = 1: 3
    subplot(3, 1, ii)
    plot(data_pred(ii, sampleorder), 'DisplayName', 'Model')
    hold on    
    plot(log(countsn(sampleorder, ii)), 'DisplayName', 'Data')
    hold off
    title(['cell type', num2str(ii)])
    xlabel('condition index')
    ylabel('proportions')
    legend()
end

% mean absolute error
mean(mean(abs(data_pred' - log(countsn))))



figure; imagesc(cur_beta(1:end,1:2))
colormap(brewermap(21,'RdBu'))
title('coefficients')
caxis([-1.5,1.5])
c=colorbar();
c.Label.String = 'coefficient value';
xtickangle(45)
set(gca,'YTick', 1:8, 'YTickLabel', {'EGF/FGF ( \beta_{k,1} )', 'BMP4 (  \beta_{k,2} )', 'PDGF-AA ( \beta_{k,3} )', 'CHIR ( \beta_{k,4} )',  'FGF9 ( \beta_{k,5} )', 'GM-CSF ( \beta_{k,6} )', 'IFNG ( \beta_{k,7} )','bias ( \beta_{k,0} ) '})
set(gca,'Xtick', 1:3, 'XTickLabel', {'AP','CN','NP'})
set(gcf, 'Position',[440   525   315   273]);
set(gca,'FontSize',16);



figure;

subplot(1,2,1)
imagesc(grow_vecn(1: 7, :) - grow_vecn(1: 7, 3))
colormap(brewermap(21,'RdBu'))

caxis([-1.5,1.5])
c=colorbar();
c.Label.String = 'coefficient value';
xtickangle(45)
set(gca,'YTick', 1:8, 'YTickLabel', {'EGF/FGF ( \beta_{k,1} )', 'BMP4 (  \beta_{k,2} )', 'PDGF-AA ( \beta_{k,3} )', 'CHIR ( \beta_{k,4} )',  'FGF9 ( \beta_{k,5} )', 'GM-CSF ( \beta_{k,6} )', 'IFNG ( \beta_{k,7} )','bias ( \beta_{k,0} ) '})
set(gca,'Xtick', 1:3, 'XTickLabel', {'AP','CN','NP'})
title('LS model')
ylabel('impact of signals')

subplot(1,2,2)
imagesc(cur_beta(:, 1: 7)')
caxis([- 1.5, 1.5])
colorbar()
title('log-linear model')
xlabel('cell type')
ylabel('impact of signals')
